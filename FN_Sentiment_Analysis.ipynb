{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Custom Code imports\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/briankalinowski/PycharmProjects/FakeNewsChallenge/Code')\n",
    "import SentimentAnalysisUtils as sentUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake & Real News Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_headline</th>\n",
       "      <th>tokenized_content</th>\n",
       "      <th>type</th>\n",
       "      <th>valid_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Muslims BUSTED They Stole Millions In Govt Ben...</td>\n",
       "      <td>Print They should pay all the back all the mon...</td>\n",
       "      <td>muslims bust steal millions in govt benefit</td>\n",
       "      <td>print should pay all the back all the money pl...</td>\n",
       "      <td>bias</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Re Why Did Attorney General Loretta Lynch Plea...</td>\n",
       "      <td>Why Did Attorney General Loretta Lynch Plead T...</td>\n",
       "      <td>re why do attorney general loretta lynch plead...</td>\n",
       "      <td>why do attorney general loretta lynch plead th...</td>\n",
       "      <td>bias</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BREAKING Weiner Cooperating With FBI On Hillar...</td>\n",
       "      <td>Red State Fox News Sunday reported this mornin...</td>\n",
       "      <td>break weiner cooperate with fbi on hillary ema...</td>\n",
       "      <td>red state fox news sunday report this morning ...</td>\n",
       "      <td>bias</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...</td>\n",
       "      <td>Email Kayla Mueller was a prisoner and torture...</td>\n",
       "      <td>pin drop speech by father of daughter kidnappe...</td>\n",
       "      <td>email kayla mueller be a prisoner and torture ...</td>\n",
       "      <td>bias</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FANTASTIC! TRUMPS 7 POINT PLAN To Reform Healt...</td>\n",
       "      <td>Email HEALTHCARE REFORM TO MAKE AMERICA GREAT ...</td>\n",
       "      <td>fantastic trump 7 point plan to reform healthc...</td>\n",
       "      <td>email healthcare reform to make america great ...</td>\n",
       "      <td>bias</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Muslims BUSTED They Stole Millions In Govt Ben...   \n",
       "1  Re Why Did Attorney General Loretta Lynch Plea...   \n",
       "2  BREAKING Weiner Cooperating With FBI On Hillar...   \n",
       "3  PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...   \n",
       "4  FANTASTIC! TRUMPS 7 POINT PLAN To Reform Healt...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Print They should pay all the back all the mon...   \n",
       "1  Why Did Attorney General Loretta Lynch Plead T...   \n",
       "2  Red State Fox News Sunday reported this mornin...   \n",
       "3  Email Kayla Mueller was a prisoner and torture...   \n",
       "4  Email HEALTHCARE REFORM TO MAKE AMERICA GREAT ...   \n",
       "\n",
       "                                  tokenized_headline  \\\n",
       "0        muslims bust steal millions in govt benefit   \n",
       "1  re why do attorney general loretta lynch plead...   \n",
       "2  break weiner cooperate with fbi on hillary ema...   \n",
       "3  pin drop speech by father of daughter kidnappe...   \n",
       "4  fantastic trump 7 point plan to reform healthc...   \n",
       "\n",
       "                                   tokenized_content  type  valid_score  \n",
       "0  print should pay all the back all the money pl...  bias            0  \n",
       "1  why do attorney general loretta lynch plead th...  bias            0  \n",
       "2  red state fox news sunday report this morning ...  bias            0  \n",
       "3  email kayla mueller be a prisoner and torture ...  bias            0  \n",
       "4  email healthcare reform to make america great ...  bias            0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = pd.read_csv('/Users/briankalinowski/Desktop/Data/news_content_lemma.csv')\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preform Sentiment Scoring\n",
    "\n",
    "- `SentimentIntensityAnalyzer()` Returns a dict of sentiment percentage scores for each article.\n",
    "\n",
    "\n",
    "- Sentiment Scoring features are: `neg`, `neu`, `pos`, `compound`\n",
    "\n",
    "\n",
    "- The `sentiment_score` feature is extracted from the `compound` score which is a metric that calculates the sum of all the lexicon ratings which have been normalized between -1 (most extreme negative) and +1 (most extreme positive).\n",
    "\n",
    "\n",
    "    - Positive sentiment: (sentiment_score = 1), (compound score >= 0.05)\n",
    "    - Neutral sentiment: (sentiment_score = 0), (compound score > -0.05) and (compound score < 0.05)\n",
    "    - Negative sentiment: (sentiment_score = -1), (compound score <= -0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/briankalinowski/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "news_sentiment_df = sentUtils.get_sentiment_vader_scores(news_df, 'tokenized_content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>valid_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-0.2263</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.7533</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.9041</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.0950</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.9799</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment_score    neg    neu    pos  compound  valid_score\n",
       "0               -1  0.123  0.764  0.113   -0.2263            0\n",
       "1               -1  0.071  0.874  0.055   -0.7533            0\n",
       "2                1  0.017  0.900  0.083    0.9041            0\n",
       "3                1  0.253  0.472  0.275    0.0950            0\n",
       "4                1  0.080  0.765  0.154    0.9799            0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only using the sentiment scoring features \n",
    "news_sentiment_df = news_sentiment_df[['sentiment_score', 'neg', 'neu', 'pos', 'compound', 'valid_score']]\n",
    "news_sentiment_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest 1: Just Sentiment Scoring Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "x_train, x_test, y_train, y_test = train_test_split(news_sentiment_df.drop(columns=['valid_score']), \n",
    "                                                    news_sentiment_df.valid_score, \n",
    "                                                    test_size=0.5, \n",
    "                                                    random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   10.8s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   15.3s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   22.2s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:   29.1s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:   36.0s\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:   43.0s\n",
      "[Parallel(n_jobs=-1)]: Done  85 out of  90 | elapsed:   49.9s remaining:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:   54.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Training Parameters: {'min_samples_split': 10, 'n_estimators': 300}\n",
      "Best Training Score: 0.7921146953405018\n"
     ]
    }
   ],
   "source": [
    "rf_params = {'n_estimators': [100, 200, 300],\n",
    "             'min_samples_split': [2, 4, 8, 10, 12, 15]\n",
    "            }\n",
    "\n",
    "rf_sent_predict = sentUtils.run_random_forest_grid_search(x_train, y_train, rf_params, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fake</th>\n",
       "      <td>0.817213</td>\n",
       "      <td>0.706847</td>\n",
       "      <td>0.758034</td>\n",
       "      <td>5857.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>real</th>\n",
       "      <td>0.786655</td>\n",
       "      <td>0.872399</td>\n",
       "      <td>0.827311</td>\n",
       "      <td>7257.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.798460</td>\n",
       "      <td>0.798460</td>\n",
       "      <td>0.798460</td>\n",
       "      <td>0.79846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.801934</td>\n",
       "      <td>0.789623</td>\n",
       "      <td>0.792672</td>\n",
       "      <td>13114.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.800303</td>\n",
       "      <td>0.798460</td>\n",
       "      <td>0.796370</td>\n",
       "      <td>13114.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "fake           0.817213  0.706847  0.758034   5857.00000\n",
       "real           0.786655  0.872399  0.827311   7257.00000\n",
       "accuracy       0.798460  0.798460  0.798460      0.79846\n",
       "macro avg      0.801934  0.789623  0.792672  13114.00000\n",
       "weighted avg   0.800303  0.798460  0.796370  13114.00000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentUtils.format_classification_report(y_test, rf_sent_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predict_Fake</th>\n",
       "      <th>Predict_Real</th>\n",
       "      <th>True_Totals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True_Fake</th>\n",
       "      <td>4140</td>\n",
       "      <td>1717</td>\n",
       "      <td>5857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True_Real</th>\n",
       "      <td>926</td>\n",
       "      <td>6331</td>\n",
       "      <td>7257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Predict_Fake  Predict_Real  True_Totals\n",
       "True_Fake          4140          1717         5857\n",
       "True_Real           926          6331         7257"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentUtils.format_confusion_matrix(y_test, rf_sent_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest 2: Sentiment and LDA \n",
    "\n",
    "- First we create a word count matrix then apply the LDA transformation to that matrix. This gives us probabilities for each of our documents belonging in each of the respective LDA topics. \n",
    "\n",
    "\n",
    "\n",
    "- Next we will combine the LDA topic probabilities with the sentiment scoring data from the previous model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer Shape: (26227, 27906) \n",
      "\n",
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>valid_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002704</td>\n",
       "      <td>0.002704</td>\n",
       "      <td>0.002704</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.571110</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.002704</td>\n",
       "      <td>0.407262</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-0.2263</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.653693</td>\n",
       "      <td>0.174755</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.071526</td>\n",
       "      <td>0.096227</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.7533</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.067823</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.806241</td>\n",
       "      <td>0.030974</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.089019</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.9041</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.662286</td>\n",
       "      <td>0.054681</td>\n",
       "      <td>0.004002</td>\n",
       "      <td>0.065395</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.143301</td>\n",
       "      <td>0.054334</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.0950</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.177604</td>\n",
       "      <td>0.183452</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.634896</td>\n",
       "      <td>1</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.9799</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic_0   topic_1   topic_2   topic_3   topic_4   topic_5   topic_6  \\\n",
       "0  0.002704  0.002704  0.002704  0.002703  0.002703  0.002703  0.571110   \n",
       "1  0.000633  0.000633  0.000633  0.653693  0.174755  0.000633  0.071526   \n",
       "2  0.067823  0.000991  0.000990  0.806241  0.030974  0.000990  0.089019   \n",
       "3  0.662286  0.054681  0.004002  0.065395  0.004000  0.004000  0.143301   \n",
       "4  0.000578  0.000578  0.000578  0.000579  0.000578  0.000578  0.177604   \n",
       "\n",
       "    topic_7   topic_8   topic_9  sentiment_score    neg    neu    pos  \\\n",
       "0  0.002703  0.002704  0.407262               -1  0.123  0.764  0.113   \n",
       "1  0.096227  0.000633  0.000633               -1  0.071  0.874  0.055   \n",
       "2  0.000990  0.000990  0.000990                1  0.017  0.900  0.083   \n",
       "3  0.054334  0.004000  0.004001                1  0.253  0.472  0.275   \n",
       "4  0.183452  0.000578  0.634896                1  0.080  0.765  0.154   \n",
       "\n",
       "   compound  valid_score  \n",
       "0   -0.2263            0  \n",
       "1   -0.7533            0  \n",
       "2    0.9041            0  \n",
       "3    0.0950            0  \n",
       "4    0.9799            0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run CountVectorizer transformation \n",
    "vectorized_tokens = sentUtils.get_count_vectorizer_matrix(news_df, 'tokenized_content')\n",
    "\n",
    "# Get LDA transformed Topics df\n",
    "news_lda_topics = sentUtils.get_lda_transformed_topics(vectorized_tokens)\n",
    "\n",
    "# Combine with Senitment scoring df\n",
    "news_sentiment_lda_df = pd.concat([news_lda_topics, news_sentiment_df], axis=1)\n",
    "news_sentiment_lda_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split \n",
    "x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(news_sentiment_lda_df.drop(columns=['valid_score']), \n",
    "                                                            news_sentiment_lda_df.valid_score, \n",
    "                                                            test_size=0.5, \n",
    "                                                            random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   14.5s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   23.1s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   35.3s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   47.8s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 118 out of 120 | elapsed:  2.7min remaining:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:  2.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Training Parameters: {'min_samples_split': 4, 'n_estimators': 300}\n",
      "Best Training Score: 0.897963852665294\n"
     ]
    }
   ],
   "source": [
    "rf_params = {'n_estimators': [100, 200, 300, 400],\n",
    "             'min_samples_split': [2, 4, 8, 10, 12, 15]\n",
    "            }\n",
    "\n",
    "rf_lda_predict, rf_lda_probs = sentUtils.run_random_forest_grid_search(x_train_2, y_train_2, rf_params, x_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predict_Fake</th>\n",
       "      <th>Predict_Real</th>\n",
       "      <th>True_Totals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True_Fake</th>\n",
       "      <td>4934</td>\n",
       "      <td>923</td>\n",
       "      <td>5857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True_Real</th>\n",
       "      <td>353</td>\n",
       "      <td>6904</td>\n",
       "      <td>7257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Predict_Fake  Predict_Real  True_Totals\n",
       "True_Fake          4934           923         5857\n",
       "True_Real           353          6904         7257"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentUtils.format_confusion_matrix(y_test_2, rf_lda_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fake</th>\n",
       "      <td>0.933232</td>\n",
       "      <td>0.842411</td>\n",
       "      <td>0.885499</td>\n",
       "      <td>5857.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>real</th>\n",
       "      <td>0.882075</td>\n",
       "      <td>0.951357</td>\n",
       "      <td>0.915407</td>\n",
       "      <td>7257.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.902699</td>\n",
       "      <td>0.902699</td>\n",
       "      <td>0.902699</td>\n",
       "      <td>0.902699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.907654</td>\n",
       "      <td>0.896884</td>\n",
       "      <td>0.900453</td>\n",
       "      <td>13114.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.904923</td>\n",
       "      <td>0.902699</td>\n",
       "      <td>0.902049</td>\n",
       "      <td>13114.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score       support\n",
       "fake           0.933232  0.842411  0.885499   5857.000000\n",
       "real           0.882075  0.951357  0.915407   7257.000000\n",
       "accuracy       0.902699  0.902699  0.902699      0.902699\n",
       "macro avg      0.907654  0.896884  0.900453  13114.000000\n",
       "weighted avg   0.904923  0.902699  0.902049  13114.000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentUtils.format_classification_report(y_test_2, rf_lda_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save RF2 Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_rf_test = x_test_2\n",
    "sentiment_rf_test['type'] = news_df.reindex(x_test_2.index).type\n",
    "sentiment_rf_test['valid_score'] = y_test_2\n",
    "sentiment_rf_test['valid_prediction'] = rf_lda_predict\n",
    "\n",
    "sentiment_rf_test = sentiment_rf_test.reset_index(drop=True)\n",
    "\n",
    "rf_probs_df = pd.DataFrame(rf_lda_probs, columns=['fake_prob', 'real_prob'])\n",
    "sentiment_rf_test = pd.concat([sentiment_rf_test, rf_probs_df], axis=1)\n",
    "sentiment_rf_test['score_abs'] = sentiment_rf_test.apply((lambda row: abs(row.fake_prob - row.real_prob)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "      <th>...</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>valid_score</th>\n",
       "      <th>valid_prediction</th>\n",
       "      <th>type</th>\n",
       "      <th>fake_prob</th>\n",
       "      <th>real_prob</th>\n",
       "      <th>score_abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.079851</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.047413</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.012786</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.613935</td>\n",
       "      <td>0.244941</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>bs</td>\n",
       "      <td>0.498889</td>\n",
       "      <td>0.501111</td>\n",
       "      <td>0.002222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.195934</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.090281</td>\n",
       "      <td>0.148315</td>\n",
       "      <td>0.006419</td>\n",
       "      <td>0.143316</td>\n",
       "      <td>0.351773</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.063216</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.081</td>\n",
       "      <td>-0.9978</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>National Review</td>\n",
       "      <td>0.080611</td>\n",
       "      <td>0.919389</td>\n",
       "      <td>0.838778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.028693</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.016483</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.103363</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.089566</td>\n",
       "      <td>0.204958</td>\n",
       "      <td>0.013935</td>\n",
       "      <td>0.542269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.9880</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>National Review</td>\n",
       "      <td>0.031333</td>\n",
       "      <td>0.968667</td>\n",
       "      <td>0.937333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.066610</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002633</td>\n",
       "      <td>0.912334</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bs</td>\n",
       "      <td>0.993333</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.986667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.138528</td>\n",
       "      <td>0.243296</td>\n",
       "      <td>0.021948</td>\n",
       "      <td>0.353679</td>\n",
       "      <td>0.103796</td>\n",
       "      <td>0.037006</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.101182</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Vox</td>\n",
       "      <td>0.501651</td>\n",
       "      <td>0.498349</td>\n",
       "      <td>0.003302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic_0   topic_1   topic_2   topic_3   topic_4   topic_5   topic_6  \\\n",
       "0  0.079851  0.000215  0.047413  0.000215  0.000215  0.000215  0.012786   \n",
       "1  0.195934  0.000249  0.090281  0.148315  0.006419  0.143316  0.351773   \n",
       "2  0.028693  0.000244  0.016483  0.000244  0.103363  0.000244  0.089566   \n",
       "3  0.002632  0.002632  0.002632  0.002632  0.066610  0.002632  0.002632   \n",
       "4  0.138528  0.243296  0.021948  0.353679  0.103796  0.037006  0.000188   \n",
       "\n",
       "    topic_7   topic_8   topic_9  ...    neg    neu    pos  compound  \\\n",
       "0  0.000215  0.613935  0.244941  ...  0.083  0.761  0.156    0.9948   \n",
       "1  0.000249  0.063216  0.000249  ...  0.168  0.751  0.081   -0.9978   \n",
       "2  0.204958  0.013935  0.542269  ...  0.061  0.828  0.110    0.9880   \n",
       "3  0.002632  0.002633  0.912334  ...  0.052  0.896  0.053    0.0258   \n",
       "4  0.101182  0.000188  0.000188  ...  0.086  0.778  0.136    0.9913   \n",
       "\n",
       "   valid_score  valid_prediction             type fake_prob  real_prob  \\\n",
       "0            0                 1               bs  0.498889   0.501111   \n",
       "1            1                 1  National Review  0.080611   0.919389   \n",
       "2            1                 1  National Review  0.031333   0.968667   \n",
       "3            0                 0               bs  0.993333   0.006667   \n",
       "4            1                 0              Vox  0.501651   0.498349   \n",
       "\n",
       "   score_abs  \n",
       "0   0.002222  \n",
       "1   0.838778  \n",
       "2   0.937333  \n",
       "3   0.986667  \n",
       "4   0.003302  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_rf_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_rf_test.to_csv('/Users/briankalinowski/Desktop/Data/sentiment_lda_test.csv', header=True, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
