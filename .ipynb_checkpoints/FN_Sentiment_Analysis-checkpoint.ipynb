{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Custom Code imports\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/briankalinowski/PycharmProjects/FakeNewsChallenge/Code')\n",
    "import SentimentAnalysisUtils as sentUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake & Real News Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_headline</th>\n",
       "      <th>tokenized_content</th>\n",
       "      <th>type</th>\n",
       "      <th>valid_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Muslims BUSTED They Stole Millions In Govt Ben...</td>\n",
       "      <td>Print They should pay all the back all the mon...</td>\n",
       "      <td>muslims bust steal millions in govt benefit</td>\n",
       "      <td>print should pay all the back all the money pl...</td>\n",
       "      <td>bias</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Re Why Did Attorney General Loretta Lynch Plea...</td>\n",
       "      <td>Why Did Attorney General Loretta Lynch Plead T...</td>\n",
       "      <td>re why do attorney general loretta lynch plead...</td>\n",
       "      <td>why do attorney general loretta lynch plead th...</td>\n",
       "      <td>bias</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BREAKING Weiner Cooperating With FBI On Hillar...</td>\n",
       "      <td>Red State Fox News Sunday reported this mornin...</td>\n",
       "      <td>break weiner cooperate with fbi on hillary ema...</td>\n",
       "      <td>red state fox news sunday report this morning ...</td>\n",
       "      <td>bias</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...</td>\n",
       "      <td>Email Kayla Mueller was a prisoner and torture...</td>\n",
       "      <td>pin drop speech by father of daughter kidnappe...</td>\n",
       "      <td>email kayla mueller be a prisoner and torture ...</td>\n",
       "      <td>bias</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FANTASTIC! TRUMPS 7 POINT PLAN To Reform Healt...</td>\n",
       "      <td>Email HEALTHCARE REFORM TO MAKE AMERICA GREAT ...</td>\n",
       "      <td>fantastic trump 7 point plan to reform healthc...</td>\n",
       "      <td>email healthcare reform to make america great ...</td>\n",
       "      <td>bias</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Muslims BUSTED They Stole Millions In Govt Ben...   \n",
       "1  Re Why Did Attorney General Loretta Lynch Plea...   \n",
       "2  BREAKING Weiner Cooperating With FBI On Hillar...   \n",
       "3  PIN DROP SPEECH BY FATHER OF DAUGHTER Kidnappe...   \n",
       "4  FANTASTIC! TRUMPS 7 POINT PLAN To Reform Healt...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Print They should pay all the back all the mon...   \n",
       "1  Why Did Attorney General Loretta Lynch Plead T...   \n",
       "2  Red State Fox News Sunday reported this mornin...   \n",
       "3  Email Kayla Mueller was a prisoner and torture...   \n",
       "4  Email HEALTHCARE REFORM TO MAKE AMERICA GREAT ...   \n",
       "\n",
       "                                  tokenized_headline  \\\n",
       "0        muslims bust steal millions in govt benefit   \n",
       "1  re why do attorney general loretta lynch plead...   \n",
       "2  break weiner cooperate with fbi on hillary ema...   \n",
       "3  pin drop speech by father of daughter kidnappe...   \n",
       "4  fantastic trump 7 point plan to reform healthc...   \n",
       "\n",
       "                                   tokenized_content  type  valid_score  \n",
       "0  print should pay all the back all the money pl...  bias            0  \n",
       "1  why do attorney general loretta lynch plead th...  bias            0  \n",
       "2  red state fox news sunday report this morning ...  bias            0  \n",
       "3  email kayla mueller be a prisoner and torture ...  bias            0  \n",
       "4  email healthcare reform to make america great ...  bias            0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = pd.read_csv('/Users/briankalinowski/Desktop/Data/news_content_lemma.csv')\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preform Sentiment Scoring\n",
    "\n",
    "- `SentimentIntensityAnalyzer()` Returns a dict of sentiment percentage scores for each article.\n",
    "\n",
    "\n",
    "- Sentiment Scoring features are: `neg`, `neu`, `pos`, `compound`\n",
    "\n",
    "\n",
    "- The `sentiment_score` feature is extracted from the `compound` score which is a metric that calculates the sum of all the lexicon ratings which have been normalized between -1 (most extreme negative) and +1 (most extreme positive).\n",
    "\n",
    "\n",
    "    - Positive sentiment: (sentiment_score = 1), (compound score >= 0.05)\n",
    "    - Neutral sentiment: (sentiment_score = 0), (compound score > -0.05) and (compound score < 0.05)\n",
    "    - Negative sentiment: (sentiment_score = -1), (compound score <= -0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/briankalinowski/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "news_sentiment_df = sentUtils.get_sentiment_vader_scores(news_df, 'tokenized_content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>valid_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-0.2263</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.7533</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.9041</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.0950</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.9799</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment_score    neg    neu    pos  compound  valid_score\n",
       "0               -1  0.123  0.764  0.113   -0.2263            0\n",
       "1               -1  0.071  0.874  0.055   -0.7533            0\n",
       "2                1  0.017  0.900  0.083    0.9041            0\n",
       "3                1  0.253  0.472  0.275    0.0950            0\n",
       "4                1  0.080  0.765  0.154    0.9799            0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only using the sentiment scoring features \n",
    "news_sentiment_df = news_sentiment_df[['sentiment_score', 'neg', 'neu', 'pos', 'compound', 'valid_score']]\n",
    "news_sentiment_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest 1: Just Sentiment Scoring Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "x_train, x_test, y_train, y_test = train_test_split(news_sentiment_df.drop(columns=['valid_score']), \n",
    "                                                    news_sentiment_df.valid_score, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   14.1s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   18.2s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   26.6s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   39.2s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:   51.2s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  85 out of  90 | elapsed:  1.5min remaining:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Training Parameters: {'min_samples_split': 15, 'n_estimators': 300}\n",
      "Best Training Score: 0.8019636814260521\n"
     ]
    }
   ],
   "source": [
    "rf_params = {'n_estimators': [100, 200, 300],\n",
    "             'min_samples_split': [2, 4, 8, 10, 12, 15]\n",
    "            }\n",
    "\n",
    "rf_sent_predict = sentUtils.run_random_forest_grid_search(x_train, y_train, rf_params, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fake</th>\n",
       "      <td>0.844836</td>\n",
       "      <td>0.707894</td>\n",
       "      <td>0.770326</td>\n",
       "      <td>2369.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>real</th>\n",
       "      <td>0.787795</td>\n",
       "      <td>0.892944</td>\n",
       "      <td>0.837080</td>\n",
       "      <td>2877.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.809379</td>\n",
       "      <td>0.809379</td>\n",
       "      <td>0.809379</td>\n",
       "      <td>0.809379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.816316</td>\n",
       "      <td>0.800419</td>\n",
       "      <td>0.803703</td>\n",
       "      <td>5246.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.813554</td>\n",
       "      <td>0.809379</td>\n",
       "      <td>0.806935</td>\n",
       "      <td>5246.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "fake           0.844836  0.707894  0.770326  2369.000000\n",
       "real           0.787795  0.892944  0.837080  2877.000000\n",
       "accuracy       0.809379  0.809379  0.809379     0.809379\n",
       "macro avg      0.816316  0.800419  0.803703  5246.000000\n",
       "weighted avg   0.813554  0.809379  0.806935  5246.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentUtils.format_classification_report(y_test, rf_sent_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predict_Fake</th>\n",
       "      <th>Predict_Real</th>\n",
       "      <th>True_Totals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True_Fake</th>\n",
       "      <td>1677</td>\n",
       "      <td>692</td>\n",
       "      <td>2369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True_Real</th>\n",
       "      <td>308</td>\n",
       "      <td>2569</td>\n",
       "      <td>2877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Predict_Fake  Predict_Real  True_Totals\n",
       "True_Fake          1677           692         2369\n",
       "True_Real           308          2569         2877"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentUtils.format_confusion_matrix(y_test, rf_sent_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest 2: Sentiment and LDA \n",
    "\n",
    "- First we create a word count matrix then apply the LDA transformation to that matrix. This gives us probabilities for each of our documents belonging in each of the respective LDA topics. \n",
    "\n",
    "\n",
    "\n",
    "- Next we will combine the LDA topic probabilities with the sentiment scoring data from the previous model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer Shape: (26227, 27906) \n",
      "\n",
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>valid_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002704</td>\n",
       "      <td>0.002704</td>\n",
       "      <td>0.002704</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.571110</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.002704</td>\n",
       "      <td>0.407262</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-0.2263</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.653693</td>\n",
       "      <td>0.174755</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.071526</td>\n",
       "      <td>0.096227</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.7533</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.067823</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.806241</td>\n",
       "      <td>0.030974</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.089019</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.9041</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.662286</td>\n",
       "      <td>0.054681</td>\n",
       "      <td>0.004002</td>\n",
       "      <td>0.065395</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.143301</td>\n",
       "      <td>0.054334</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.0950</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.177604</td>\n",
       "      <td>0.183452</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.634896</td>\n",
       "      <td>1</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.9799</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic_0   topic_1   topic_2   topic_3   topic_4   topic_5   topic_6  \\\n",
       "0  0.002704  0.002704  0.002704  0.002703  0.002703  0.002703  0.571110   \n",
       "1  0.000633  0.000633  0.000633  0.653693  0.174755  0.000633  0.071526   \n",
       "2  0.067823  0.000991  0.000990  0.806241  0.030974  0.000990  0.089019   \n",
       "3  0.662286  0.054681  0.004002  0.065395  0.004000  0.004000  0.143301   \n",
       "4  0.000578  0.000578  0.000578  0.000579  0.000578  0.000578  0.177604   \n",
       "\n",
       "    topic_7   topic_8   topic_9  sentiment_score    neg    neu    pos  \\\n",
       "0  0.002703  0.002704  0.407262               -1  0.123  0.764  0.113   \n",
       "1  0.096227  0.000633  0.000633               -1  0.071  0.874  0.055   \n",
       "2  0.000990  0.000990  0.000990                1  0.017  0.900  0.083   \n",
       "3  0.054334  0.004000  0.004001                1  0.253  0.472  0.275   \n",
       "4  0.183452  0.000578  0.634896                1  0.080  0.765  0.154   \n",
       "\n",
       "   compound  valid_score  \n",
       "0   -0.2263            0  \n",
       "1   -0.7533            0  \n",
       "2    0.9041            0  \n",
       "3    0.0950            0  \n",
       "4    0.9799            0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run CountVectorizer transformation \n",
    "vectorized_tokens = sentUtils.get_count_vectorizer_matrix(news_df, 'tokenized_content')\n",
    "\n",
    "# Get LDA transformed Topics df\n",
    "news_lda_topics = sentUtils.get_lda_transformed_topics(vectorized_tokens)\n",
    "\n",
    "# Combine with Senitment scoring df\n",
    "news_sentiment_lda_df = pd.concat([news_lda_topics, news_sentiment_df], axis=1)\n",
    "news_sentiment_lda_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split \n",
    "x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(news_sentiment_lda_df.drop(columns=['valid_score']), \n",
    "                                                            news_sentiment_lda_df.valid_score, \n",
    "                                                            test_size=0.2, \n",
    "                                                            random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   22.2s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   29.7s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   45.3s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done  85 out of  90 | elapsed:  2.7min remaining:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:  2.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Training Parameters: {'min_samples_split': 2, 'n_estimators': 300}\n",
      "Best Training Score: 0.9029121586197035\n"
     ]
    }
   ],
   "source": [
    "rf_lda_predict = sentUtils.run_random_forest_grid_search(x_train_2, y_train_2, rf_params, x_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predict_Fake</th>\n",
       "      <th>Predict_Real</th>\n",
       "      <th>True_Totals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True_Fake</th>\n",
       "      <td>2012</td>\n",
       "      <td>357</td>\n",
       "      <td>2369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True_Real</th>\n",
       "      <td>123</td>\n",
       "      <td>2754</td>\n",
       "      <td>2877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Predict_Fake  Predict_Real  True_Totals\n",
       "True_Fake          2012           357         2369\n",
       "True_Real           123          2754         2877"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentUtils.format_confusion_matrix(y_test_2, rf_lda_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fake</th>\n",
       "      <td>0.942389</td>\n",
       "      <td>0.849304</td>\n",
       "      <td>0.893428</td>\n",
       "      <td>2369.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>real</th>\n",
       "      <td>0.885246</td>\n",
       "      <td>0.957247</td>\n",
       "      <td>0.919840</td>\n",
       "      <td>2877.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.908502</td>\n",
       "      <td>0.908502</td>\n",
       "      <td>0.908502</td>\n",
       "      <td>0.908502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.913817</td>\n",
       "      <td>0.903275</td>\n",
       "      <td>0.906634</td>\n",
       "      <td>5246.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.911051</td>\n",
       "      <td>0.908502</td>\n",
       "      <td>0.907913</td>\n",
       "      <td>5246.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "fake           0.942389  0.849304  0.893428  2369.000000\n",
       "real           0.885246  0.957247  0.919840  2877.000000\n",
       "accuracy       0.908502  0.908502  0.908502     0.908502\n",
       "macro avg      0.913817  0.903275  0.906634  5246.000000\n",
       "weighted avg   0.911051  0.908502  0.907913  5246.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentUtils.format_classification_report(y_test_2, rf_lda_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
